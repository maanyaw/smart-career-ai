# -*- coding: utf-8 -*-
"""SmartCareer AI .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15mX5Go3wNj0O6ClO7DS-E69iJHvHx9x0
"""

# =======================
# STEP 1: Load & Explore Resumes Dataset
# =======================

import pandas as pd

# Load all CSV files
people_df = pd.read_csv("01_people.csv")
abilities_df = pd.read_csv("02_abilities.csv")
education_df = pd.read_csv("03_education.csv")
experience_df = pd.read_csv("04_experience.csv")
person_skills_df = pd.read_csv("05_person_skills.csv")
skills_df = pd.read_csv("06_skills.csv")

# -----------------------
# 1. Show structure (columns)
# -----------------------
print("People Columns:", people_df.columns.tolist())
print("Education Columns:", education_df.columns.tolist())
print("Experience Columns:", experience_df.columns.tolist())
print("Person-Skills Columns:", person_skills_df.columns.tolist())
print("Skills Columns:", skills_df.columns.tolist())

# -----------------------
# 2. Preview samples
# -----------------------
print("\nSample People Data:")
display(people_df.head(3))

print("\nSample Education Data:")
display(education_df.head(3))

print("\nSample Experience Data:")
display(experience_df.head(3))

print("\nSample Person-Skills Data:")
display(person_skills_df.head(3))

print("\nSample Skills Data:")
display(skills_df.head(3))

# -----------------------
# 3. Quick missing values check
# -----------------------
for name, df in {
    "People": people_df,
    "Education": education_df,
    "Experience": experience_df,
    "Person-Skills": person_skills_df,
    "Skills": skills_df
}.items():
    print(f"\n{name} Missing Values:")
    print(df.isnull().sum())

# -----------------------
# 4. Ensure correct datatypes for dates
# -----------------------
education_df["start_date"] = pd.to_datetime(education_df["start_date"], errors="coerce")
experience_df["start_date"] = pd.to_datetime(experience_df["start_date"], errors="coerce")
experience_df["end_date"] = pd.to_datetime(experience_df["end_date"], errors="coerce")

print("\nData types adjusted for date fields in Education & Experience.")

# =======================
# STEP 2: Merge & Clean Skills
# =======================

# Normalize skills first (for consistent matching)
person_skills_df['skill'] = person_skills_df['skill'].str.lower().str.strip()
skills_df['skill'] = skills_df['skill'].str.lower().str.strip()

# Merge with master skills list (to validate skills exist)
# Keep only skills found in the master list
person_skills_df = person_skills_df.merge(skills_df, on='skill', how='inner')

# Drop duplicate (person_id, skill) pairs
person_skills_df = person_skills_df.drop_duplicates(subset=['person_id', 'skill'])

print("✅ Cleaned Person-Skills shape:", person_skills_df.shape)
print("✅ Example cleaned skills:")
display(person_skills_df.head(10))

# =======================
# STEP 3: Create Sample Parsed Resumes
# =======================

import json
import pandas as pd

# Select a few person IDs (first 3 for demo)
sample_ids = people_df['person_id'].head(3).tolist()
parsed_resumes = []

for pid in sample_ids:
    # Get person row
    person = people_df[people_df['person_id'] == pid].iloc[0]

    # --- Skills ---
    skills = person_skills_df[person_skills_df['person_id'] == pid]['skill'].tolist()
    skills = sorted(set([s for s in skills if pd.notna(s)]))  # deduplicate + clean NaN

    # --- Education ---
    edu = education_df[education_df['person_id'] == pid][
        ['institution', 'program', 'start_date', 'location']
    ].to_dict(orient='records')

    # Normalize dates in education and replace NaT with None
    for e in edu:
        if pd.notna(e['start_date']):
            e['start_date'] = str(e['start_date'])[:7]  # keep YYYY-MM format
        else:
            e['start_date'] = None

    # --- Experience ---
    exp = experience_df[experience_df['person_id'] == pid][
        ['firm', 'title', 'start_date', 'end_date', 'location']
    ].to_dict(orient='records')

    # Normalize dates in experience and replace NaT with None
    for ex in exp:
        if pd.notna(ex['start_date']):
            ex['start_date'] = str(ex['start_date'])[:7]
        else:
            ex['start_date'] = None
        if pd.notna(ex['end_date']):
            ex['end_date'] = str(ex['end_date'])[:7]
        else:
            ex['end_date'] = None

    # --- Abilities ---
    abilities = []
    if 'ability' in abilities_df.columns:
        abilities = abilities_df[abilities_df['person_id'] == pid]['ability'].dropna().tolist()

    # --- Assemble resume ---
    resume_dict = {
        'name': str(person['name']) if pd.notna(person['name']) else "",
        'email': str(person['email']) if pd.notna(person['email']) else "",
        'skills': skills,
        'education': edu,
        'experience': exp,
        'abilities': abilities
    }

    parsed_resumes.append(resume_dict)

# View sample parsed resumes
print(json.dumps(parsed_resumes, indent=2, ensure_ascii=False))

# =======================
# STEP 4: Load & Clean Job Roles Dataset
# =======================

import pandas as pd

# Load Job Roles dataset with encoding fallback
try:
    jd_df = pd.read_csv("/content/IT_Job_Roles_Skills.csv", encoding="utf-8")
except UnicodeDecodeError:
    jd_df = pd.read_csv("/content/IT_Job_Roles_Skills.csv", encoding="latin-1")

# -----------------------
# 1. Show structure
# -----------------------
print("Columns available:", jd_df.columns.tolist())
print("\nRaw sample:")
display(jd_df.head(3))

# -----------------------
# 2. Normalize text
# -----------------------
jd_df['Skills'] = jd_df['Skills'].str.lower().str.strip()
jd_df['Certifications'] = jd_df['Certifications'].str.lower().str.strip()

# -----------------------
# 3. Split skills/certs into lists
# -----------------------
jd_df['Skills'] = jd_df['Skills'].apply(
    lambda x: [s.strip() for s in str(x).split(',')] if pd.notna(x) else []
)
jd_df['Certifications'] = jd_df['Certifications'].apply(
    lambda x: [c.strip() for c in str(x).split(',')] if pd.notna(x) else []
)

# -----------------------
# 4. Preview cleaned data
# -----------------------
print("\n✅ Cleaned Job Roles data sample:")
display(jd_df.head(3))

# =======================
# STEP 5: Clean & Normalize JD Skills for Matching
# =======================

import re

# Ensure column names are lowercase with underscores
jd_df.rename(columns=lambda x: x.strip().lower().replace(' ', '_'), inplace=True)

# Define important columns
skill_col = 'skills'
role_col = 'job_title'

# Drop rows with missing job titles or skills
jd_df = jd_df.dropna(subset=[skill_col, role_col])

# Function to clean and normalize skills
def clean_skills(skill_str):
    # Split on commas, strip whitespace, remove stray quotes/brackets, keep multi-word intact
    skills = [
        re.sub(r"[\[\]'\" ]", "", s).lower().strip()
        for s in str(skill_str).split(',')
        if s.strip()
    ]
    return sorted(set(skills))  # deduplicate + sort

# Apply cleaning to JD skills
jd_df[skill_col] = jd_df[skill_col].apply(clean_skills)

# Add skill count column (useful for similarity calculation later)
jd_df['skill_count'] = jd_df[skill_col].apply(len)

# Remove roles with empty skill sets
jd_df = jd_df[jd_df[skill_col].map(len) > 0].reset_index(drop=True)

# Optional: sample subset for quick testing
sample_jds = jd_df[[role_col, skill_col, 'skill_count']].head(15).reset_index(drop=True)

# -----------------------
# Preview cleaned roles
# -----------------------
for index, row in sample_jds.iterrows():
    print(f"\n🔹 Role: {row[role_col]}")
    print(f"   ➤ Skills ({row['skill_count']}): {row[skill_col]}")

# =======================
# STEP 7: Resume-to-Job Role Matching (with Skill Gap Analysis)
# =======================

def jaccard_similarity(list1, list2):
    set1, set2 = set(list1), set(list2)
    intersection = len(set1.intersection(set2))
    union = len(set1.union(set2))
    return intersection / union if union != 0 else 0

# Normalize resume skills (same as JD cleaning)
def normalize_skills(skills):
    return [s.strip().lower() for s in skills if s and isinstance(s, str)]

# Iterate over parsed resumes
for resume in parsed_resumes:
    print(f"\n📄 Resume: {resume['name']}")
    resume_skills = normalize_skills(resume['skills'])

    role_scores = []
    for _, row in sample_jds.iterrows():
        role = row[role_col]
        jd_skills = row[skill_col]

        score = jaccard_similarity(resume_skills, jd_skills)
        if score > 0:  # skip roles with no overlap
            matched = set(jd_skills).intersection(resume_skills)
            missing = set(jd_skills) - set(resume_skills)
            role_scores.append((role, score, matched, missing))

    # Sort by similarity score
    top_roles = sorted(role_scores, key=lambda x: x[1], reverse=True)[:3]

    # Display results
    print("🎯 Top Job Role Matches:")
    for role, score, matched, missing in top_roles:
        print(f" - {role} ({round(score * 100, 2)}% match)")
        print(f"    ✔ Matched Skills: {sorted(matched)}")
        print(f"    ❌ Missing Skills: {sorted(missing)}")

# Re-run this if needed to ensure resume_skills and sample_jds are loaded
def skill_gap_analysis(resume_skills, required_skills):
    resume_set = set([s.lower().strip() for s in resume_skills])
    required_set = set([s.lower().strip() for s in required_skills])

    matched = sorted(list(resume_set.intersection(required_set)))
    missing = sorted(list(required_set.difference(resume_set)))

    return matched, missing

# Loop through each resume and its top 3 matches
for resume in parsed_resumes:
    print(f"\n📄 Resume: {resume['name']}")
    resume_skills = [s.lower().strip() for s in resume['skills']]

    role_scores = []
    for _, row in sample_jds.iterrows():
        role = row[role_col]
        required_skills = row[skill_col]
        score = jaccard_similarity(resume_skills, required_skills)
        role_scores.append((role, score, required_skills))

    # Top 3 roles
    top_roles = sorted(role_scores, key=lambda x: x[1], reverse=True)[:3]

    for role, score, required_skills in top_roles:
        matched, missing = skill_gap_analysis(resume_skills, required_skills)

        print(f"\n🔹 Role: {role} ({round(score * 100, 2)}% match)")
        print(f"✅ Matched Skills ({len(matched)}): {matched}")
        print(f"❌ Missing Skills ({len(missing)}): {missing}")

# =======================
# STEP 9: Skill Gap → Course Recommendations
# =======================

# Dummy course catalog (can be replaced with Coursera/edX dataset)
course_catalog = [
    {'skill': 'python', 'course_name': 'Python for Everybody', 'provider': 'Coursera', 'link': 'https://coursera.org/learn/python'},
    {'skill': 'sql', 'course_name': 'SQL for Data Science', 'provider': 'Coursera', 'link': 'https://coursera.org/learn/sql-data-science'},
    {'skill': 'power bi', 'course_name': 'Power BI Essentials', 'provider': 'Udemy', 'link': 'https://udemy.com/course/power-bi-essentials/'},
    {'skill': 'excel', 'course_name': 'Excel Skills for Business', 'provider': 'Coursera', 'link': 'https://coursera.org/learn/excel'},
    {'skill': 'tableau', 'course_name': 'Tableau A-Z', 'provider': 'Udemy', 'link': 'https://udemy.com/course/tableau-data-visualization/'},
    {'skill': 'statistics', 'course_name': 'Intro to Statistics', 'provider': 'edX', 'link': 'https://edx.org/course/statistics'}
]

# Normalize catalog skills
for course in course_catalog:
    course['skill'] = course['skill'].lower().strip()

# Build lookup dictionary
course_lookup = {c['skill']: c for c in course_catalog}

# Function: Recommend courses for missing skills
def recommend_courses(missing_skills, course_lookup):
    recs = []
    for skill in missing_skills:
        if skill in course_lookup:
            recs.append(course_lookup[skill])
    return recs

# Skill gap analysis function
def skill_gap_analysis(resume_skills, required_skills):
    resume_set = set([s.lower().strip() for s in resume_skills])
    required_set = set([s.lower().strip() for s in required_skills])

    matched = sorted(list(resume_set.intersection(required_set)))
    missing = sorted(list(required_set.difference(resume_set)))

    return matched, missing

# Loop through each resume and show role matches + recommendations
for resume in parsed_resumes:
    print(f"\n📄 Resume: {resume['name']}")
    resume_skills = [s.lower().strip() for s in resume['skills']]

    role_scores = []
    for _, row in sample_jds.iterrows():
        role = row[role_col]
        required_skills = row[skill_col]
        score = jaccard_similarity(resume_skills, required_skills)
        role_scores.append((role, score, required_skills))

    # Top 3 roles
    top_roles = sorted(role_scores, key=lambda x: x[1], reverse=True)[:3]

    for role, score, required_skills in top_roles:
        matched, missing = skill_gap_analysis(resume_skills, required_skills)

        print(f"\n🔹 Role: {role} ({round(score * 100, 2)}% match)")
        print(f"✅ Matched Skills ({len(matched)}): {matched}")
        print(f"❌ Missing Skills ({len(missing)}): {missing}")

        # Recommend courses for missing skills
        recs = recommend_courses(missing, course_lookup)
        if recs:
            print("📘 Recommended Courses:")
            for r in recs:
                print(f"   - {r['course_name']} ({r['provider']}) → {r['link']}")

# =======================
# STEP 10: Skill Gap → Course Recommendations (with Fuzzy Matching)
# =======================

from difflib import get_close_matches

# Convert catalog to DataFrame for easy lookup
course_df = pd.DataFrame(course_catalog)
course_df['skill'] = course_df['skill'].str.lower().str.strip()

# Function to find best matching course for a skill
def find_course(skill, course_df):
    # Exact match first
    exact = course_df[course_df['skill'] == skill]
    if not exact.empty:
        return exact.to_dict('records')

    # Fuzzy match if no exact match
    all_skills = course_df['skill'].tolist()
    match = get_close_matches(skill, all_skills, n=1, cutoff=0.6)  # cutoff=0.6 is a lenient threshold
    if match:
        fuzzy = course_df[course_df['skill'] == match[0]]
        return fuzzy.to_dict('records')

    return []  # no course found

# Updated Skill Gap Loop with Fuzzy Course Recommendation
for resume in parsed_resumes:
    print(f"\n📄 Resume: {resume['name']}")
    resume_skills = [s.lower().strip() for s in resume['skills']]

    role_scores = []
    for _, row in sample_jds.iterrows():
        role = row[role_col]
        required_skills = row[skill_col]
        score = jaccard_similarity(resume_skills, required_skills)
        role_scores.append((role, score, required_skills))

    top_roles = sorted(role_scores, key=lambda x: x[1], reverse=True)[:3]

    for role, score, required_skills in top_roles:
        matched, missing = skill_gap_analysis(resume_skills, required_skills)

        print(f"\n🔹 Role: {role} ({round(score * 100, 2)}% match)")
        print(f"✅ Matched Skills: {matched}")
        print(f"❌ Missing Skills: {missing}")

        # Recommend courses
        print("🎓 Course Recommendations:")
        for skill in missing:
            courses = find_course(skill, course_df)
            if courses:
                for c in courses:
                    print(f" - {skill.title()}: {c['course_name']} ({c['provider']}) → {c['link']}")
            else:
                print(f" - {skill.title()}: No course found.")

#Step 1: Unzip and Load the Dataset
import zipfile
import os

# Unzip
with zipfile.ZipFile('/content/archive (2).zip', 'r') as zip_ref:
    zip_ref.extractall('/courses_json')

# List files to check structure
files = os.listdir('/courses_json')
print("Files in dataset:", files[:5])  # Show first 5

#Step 2: import json

course_data = []

for file in files:
    if file.endswith('.json'):
        try:
            with open(f"/courses_json/{file}", 'r', encoding='utf-8') as f:
                raw = f.read().strip()
                if raw.startswith('['):
                    parsed = json.loads(raw)
                    course_data.extend(parsed)  # list of courses
                else:
                    parsed = json.loads(raw)
                    course_data.append(parsed)  # single course
        except Exception as e:
            print(f"❌ Could not parse {file}: {e}")

# Convert to DataFrame if any valid data found
if course_data:
    course_df = pd.DataFrame(course_data)
    print("✅ Loaded courses:", len(course_df))
    print(course_df.head())
else:
    print("⚠️ Still no valid courses found.")



course_df.columns.tolist()

course_df[['title', 'external_url']].dropna().to_csv("/content/courses_final.csv", index=False)

course_df = pd.read_csv("/content/courses_final.csv")

course_df.columns = ['name', 'url']

# Step 3: Inspect and Clean Course Data
# Inspect available columns
print("🔎 Columns available:", course_df.columns.tolist())
print("\n📦 Sample Row:")
print(course_df.iloc[0])

# Normalize column names
course_df.columns = [c.lower().strip().replace(' ', '_') for c in course_df.columns]

# Check columns after renaming
print(course_df.columns.tolist())

# Step 20: Clean final course dataset (name + url only)

# Drop rows where name or url is missing
course_df = course_df.dropna(subset=['name', 'url'])

# Normalize values
course_df['name'] = course_df['name'].astype(str).str.strip()
course_df['url'] = course_df['url'].astype(str).str.strip()

# Remove duplicates if any
course_df = course_df.drop_duplicates(subset=['name', 'url']).reset_index(drop=True)

# Inspect final cleaned dataset
print("✅ Cleaned course_df shape:", course_df.shape)
print(course_df.head(5))

course_df.columns.tolist()

# Step 22: Final clean + save

# Normalize column names (safe to keep)
course_df.columns = [c.lower().strip().replace(' ', '_') for c in course_df.columns]

# Drop rows where 'name' or 'url' is missing
course_df = course_df.dropna(subset=['name', 'url'])

# Normalize text
course_df['name'] = course_df['name'].astype(str).str.strip()
course_df['url'] = course_df['url'].astype(str).str.strip()

# Remove duplicates
course_df = course_df.drop_duplicates(subset=['name', 'url']).reset_index(drop=True)

# Save final dataset
course_df[['name', 'url']].to_csv("/content/courses_final.csv", index=False)

print("✅ Cleaned course dataset saved as /content/courses_final.csv")
print("📦 Shape:", course_df.shape)
print(course_df.head(5))

import random
import pandas as pd

# Step 1: Load your existing course_df if not already
try:
    course_df = pd.read_csv("/content/courses_final.csv")
except Exception as e:
    print("⚠️ Error loading existing courses_final.csv:", e)
    course_df = pd.DataFrame(columns=["name", "url", "platform"])

# Initialize full_df with course_df
full_df = course_df.copy()

# Step 2: Real curated course entries for common missing tech skills
manual_courses = [
    {"name": "Meta Front-End Developer - React", "url": "https://www.coursera.org/learn/meta-front-end-developer", "platform": "Coursera"},
    {"name": "Django for Beginners", "url": "https://www.udemy.com/course/django-for-beginners/", "platform": "Udemy"},
    {"name": "REST APIs with Flask and Python", "url": "https://www.udemy.com/course/rest-api-flask-and-python/", "platform": "Udemy"},
    {"name": "Master the Coding Interview: Data Structures", "url": "https://www.udemy.com/course/master-the-coding-interview-data-structures-algorithms/", "platform": "Udemy"},
    {"name": "Docker for DevOps", "url": "https://www.udemy.com/course/docker-mastery/", "platform": "Udemy"},
    {"name": "Version Control with Git", "url": "https://www.coursera.org/learn/introduction-git-github", "platform": "Coursera"},
    {"name": "Cloud Computing Basics (AWS)", "url": "https://www.coursera.org/learn/cloud-computing-basics", "platform": "Coursera"},
    {"name": "Full-Stack Web Development with React", "url": "https://www.coursera.org/specializations/full-stack-react", "platform": "Coursera"}
]

manual_df = pd.DataFrame(manual_courses)
manual_extra_courses = [
    {"name": "Cloud Computing Basics (Coursera - LearnQuest)", "url": "https://www.coursera.org/learn/cloud-computing-basics", "platform": "Coursera"},
    {"name": "Data Mining Specialization", "url": "https://www.coursera.org/specializations/data-mining", "platform": "Coursera"},
    {"name": "Data Visualization with Tableau", "url": "https://www.coursera.org/learn/visual-analytics", "platform": "Coursera"},
    {"name": "Hadoop Platform and Application Framework", "url": "https://www.coursera.org/learn/hadoop", "platform": "Coursera"},
    {"name": "NoSQL Database Systems", "url": "https://www.edx.org/course/nosql-database-systems", "platform": "edX"},
    {"name": "Big Data Analysis with Spark", "url": "https://www.edx.org/course/big-data-analysis-with-spark", "platform": "edX"},
    {"name": "ETL and Data Pipelines with Shell, Airflow, and Kafka", "url": "https://www.coursera.org/learn/etl-data-pipelines", "platform": "Coursera"},
    {"name": "Introduction to Statistical Analysis", "url": "https://www.edx.org/course/introduction-to-statistical-analysis", "platform": "edX"},
    {"name": "Data Modeling and Relational Database Design", "url": "https://www.coursera.org/learn/data-modeling", "platform": "Coursera"},
]

manual_df2 = pd.DataFrame(manual_extra_courses)
full_df = pd.concat([full_df, manual_df2], ignore_index=True)


# Step 3: Random dummy tech skills course generator
skills = [
    "cloud computing", "data analysis", "data mining", "data modeling", "data visualization",
    "etl", "hadoop", "nosql", "spark", "statistical analysis", "machine learning", "deep learning",
    "natural language processing", "python programming", "sql", "mongodb", "big data",
    "git", "docker", "kubernetes", "devops", "tensorflow", "scikit-learn", "pandas", "numpy",
    "r programming", "ai ethics", "llms", "prompt engineering", "azure", "aws", "gcp"
]

platforms = ["Coursera", "Udemy", "EdX", "DataCamp", "Pluralsight"]

# Step 4: Generate 100 random dummy courses
extra_courses = []
for i in range(100):
    skill = random.choice(skills)
    course = {
        "name": f"Mastering {skill.title()} - Level {random.randint(1, 3)}",
        "url": f"https://example.com/{skill.replace(' ', '-')}-course-{i+1}",
        "platform": random.choice(platforms)
    }
    extra_courses.append(course)

extra_df = pd.DataFrame(extra_courses)

# Step 5: Combine and deduplicate
full_df = pd.concat([full_df, manual_df, extra_df], ignore_index=True)
full_df.drop_duplicates(subset=['name', 'url'], inplace=True)

# Step 6: Save
full_df.to_csv("/content/courses_final.csv", index=False)
print(f"✅ Final course list saved: {len(full_df)} total courses")

import re
import pandas as pd

# Reload enriched dataset
course_df = pd.read_csv("/content/courses_final.csv")

# Ensure platform column exists (fallback if missing)
if 'platform' not in course_df.columns:
    course_df['platform'] = 'Unknown'

# Mapping to fix concatenated skill names
skill_map = {
    "cloudcomputing": "cloud computing",
    "dataanalysis": "data analysis",
    "datamining": "data mining",
    "datamodeling": "data modeling",
    "datavisualization": "data visualization",
    "statisticalanalysis": "statistical analysis"
}

# Test missing skills
for raw_skill in missing:
    # Normalize skill name
    skill = skill_map.get(raw_skill, raw_skill)
    print(f"\n🔍 Looking for courses on: {skill}")

    # Regex search for skill keyword in course names
    pattern = re.compile(rf'\b{re.escape(skill)}\b', re.IGNORECASE)
    matched_courses = course_df[course_df['name'].apply(lambda x: bool(pattern.search(str(x))))]

    if not matched_courses.empty:
        for _, row in matched_courses.head(2).iterrows():
            name = row.get('name', 'Untitled')
            link = row.get('url', 'No link')
            platform = row.get('platform', 'Unknown')
            print(f" - {skill.title()}: {name} ({platform}) → {link}")
    else:
        print(f" - No course found for: {skill}")

# Step 25: Derive 'skills' column from course names
# This will let us match skills against the course title text
course_df['skills'] = course_df['name'].astype(str).str.lower()

# Inspect sample
print("✅ Added 'skills' column")
print(course_df[['name', 'skills']].head(10))

course_df['skills'] = course_df['name'].str.lower()

course_df['name'].sample(10).tolist()

!pip install PyMuPDF

import gradio as gr
import pandas as pd
import fitz  # PyMuPDF
import re
import matplotlib.pyplot as plt

# ---------- Load Job & Course Data ----------
job_df = pd.read_csv("/content/IT_Job_Roles_Skills.csv", encoding="latin-1")
job_df.rename(columns=lambda x: x.strip().lower().replace(" ", "_"), inplace=True)
job_df = job_df.dropna(subset=["skills", "job_title"])
job_df["skills"] = job_df["skills"].apply(lambda x: [s.strip().lower() for s in x.split(",")])

course_df = pd.read_csv("/content/courses_final.csv")

# ---------- Skill Extraction ----------
def extract_skills(text):
    text = text.lower()
    text = re.sub(r"[^a-zA-Z\s]", "", text)
    words = set(text.split())

    known_skills = set()
    for skills_list in job_df["skills"]:
        if isinstance(skills_list, list):
            known_skills.update(skills_list)

    # synonym mapping
    synonym_map = {
        "github": "git",
        "rest": "rest apis",
        "restapi": "rest apis",
        "dsa": "data structures",
        "cloud": "cloud computing",
        "api": "rest apis",
    }

    matched = set()
    for word in words:
        if word in known_skills:
            matched.add(word)
        elif word in synonym_map and synonym_map[word] in known_skills:
            matched.add(synonym_map[word])

    return list(matched)

# ---------- Skill Gap Analysis ----------
def skill_gap_analysis(resume_skills, required_skills):
    resume_set = set(resume_skills)
    required_set = set(required_skills)
    matched = sorted(list(resume_set.intersection(required_set)))
    missing = sorted(list(required_set.difference(resume_set)))
    return matched, missing

# ---------- Chart Function ----------
def plot_skill_gap(matched, missing, role):
    labels = ["Matched", "Missing"]
    sizes = [len(matched), len(missing)]
    colors = ["#4CAF50", "#FF6F61"]

    fig, ax = plt.subplots()
    ax.pie(sizes, labels=labels, autopct="%1.1f%%", startangle=90, colors=colors)
    ax.set_title(f"Skill Match for {role}")
    return fig

# ---------- Main Function ----------
def analyze_resume(pdf_file):
    # Extract text
    doc = fitz.open(pdf_file.name)
    resume_text = ""
    for page in doc:
        resume_text += page.get_text()

    resume_skills = extract_skills(resume_text)

    if not resume_skills:
        return "⚠️ No recognizable skills found.", None

    role_scores = []
    for _, row in job_df.iterrows():
        role = row["job_title"]
        required_skills = row["skills"]
        matched, missing = skill_gap_analysis(resume_skills, required_skills)
        score = len(matched) / len(set(required_skills)) if required_skills else 0
        role_scores.append((role, score, matched, missing))

    # Top 3 roles
    top_roles = sorted(role_scores, key=lambda x: x[1], reverse=True)[:3]

    results_text = f"✅ Extracted Resume Skills:\n{', '.join(resume_skills)}\n\n"
    plots = []

    for role, score, matched, missing in top_roles:
        results_text += f"\n🔹 Role: {role} ({round(score*100,2)}% match)\n"
        results_text += f"✔️ Matched Skills: {', '.join(matched) if matched else 'None'}\n"
        results_text += f"❌ Missing Skills: {', '.join(missing) if missing else 'None'}\n"

        # Courses
        results_text += "🎓 Recommended Courses:\n"
        for skill in missing:
            matched_courses = course_df[course_df["name"].str.contains(skill, case=False, na=False)]
            if not matched_courses.empty:
                for _, course in matched_courses.head(2).iterrows():
                    results_text += f"- {skill.title()}: {course['name']} ({course['url']})\n"
            else:
                results_text += f"- {skill.title()}: No course found.\n"

        # Generate chart for this role
        plots.append(plot_skill_gap(matched, missing, role))

    return results_text, plots[0]  # Show text + first chart for simplicity

# ---------- Gradio UI ----------
demo = gr.Interface(
    fn=analyze_resume,
    inputs=gr.File(type="filepath", label="📄 Upload Resume (PDF)"),
    outputs=[gr.Textbox(label="Analysis"), gr.Plot(label="Skill Match Chart")],
    title="🧠  SmartCareer AI ",
    description="Upload your resume and get matched job roles, missing skills, and recommended courses with visual insights."
)

demo.launch(share=True)